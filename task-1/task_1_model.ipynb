{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1zuBUUnllpPCAAuvVJdr7KwJ9_COYtXhw","authorship_tag":"ABX9TyNBlxQcl21v6NW9p3NGNvXa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"233af25196414f58ae19927b3c42e607":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7dc26461545849c487dab0eb4a211dba","IPY_MODEL_166fd59fb4454e00b49a36a9ded68bb0","IPY_MODEL_433c2a0aa56645eaba47210f27241bcf"],"layout":"IPY_MODEL_2f0e6f8900514454a1dbbe5c7d714482"}},"7dc26461545849c487dab0eb4a211dba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5680f8ee40a44c6aba7d6ec8631b64cc","placeholder":"​","style":"IPY_MODEL_1f7a7b09bcbc4db38d6f1f15fee43ab4","value":"100%"}},"166fd59fb4454e00b49a36a9ded68bb0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1509cac5352b484ea509d916330f1083","max":1200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5bb994181cb4eaeb89bc14f91f2446d","value":1200}},"433c2a0aa56645eaba47210f27241bcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e3dca0dbe194b0fb5fb31c94f190a5e","placeholder":"​","style":"IPY_MODEL_228a3802e4354cfd9137597e499a9b15","value":" 1200/1200 [14:39&lt;00:00,  1.37it/s]"}},"2f0e6f8900514454a1dbbe5c7d714482":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5680f8ee40a44c6aba7d6ec8631b64cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f7a7b09bcbc4db38d6f1f15fee43ab4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1509cac5352b484ea509d916330f1083":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5bb994181cb4eaeb89bc14f91f2446d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e3dca0dbe194b0fb5fb31c94f190a5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"228a3802e4354cfd9137597e499a9b15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LPOtSWTPslxW"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm.auto import tqdm\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Nx-lDBxftdqK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the data\n","train_data = pd.read_json('/content/drive/MyDrive/task-1/data/train.jsonl', lines=True)\n","val_data = pd.read_json('/content/drive/MyDrive/task-1/data/val.jsonl', lines=True)\n","test_data = pd.read_json('/content/drive/MyDrive/task-1/data/test.jsonl', lines=True)\n","\n","# Combine text features into a single feature\n","train_data['combined_text'] = train_data['postText'].apply(lambda x: ' '.join(x)) + ' ' + train_data['targetParagraphs'].apply(lambda x: ' '.join(x))\n","val_data['combined_text'] = val_data['postText'].apply(lambda x: ' '.join(x)) + ' ' + val_data['targetParagraphs'].apply(lambda x: ' '.join(x))\n","test_data['combined_text'] = test_data['postText'].apply(lambda x: ' '.join(x)) + ' ' + test_data['targetParagraphs'].apply(lambda x: ' '.join(x))\n","\n","# Convert tags to strings\n","train_data['tags'] = train_data['tags'].apply(lambda x: x[0] if isinstance(x, list) else x)\n","val_data['tags'] = val_data['tags'].apply(lambda x: x[0] if isinstance(x, list) else x)\n","\n","# Encode the target labels\n","label_encoder = LabelEncoder()\n","train_data['label'] = label_encoder.fit_transform(train_data['tags'])\n","val_data['label'] = label_encoder.transform(val_data['tags'])\n","\n","# Initialize BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize the input text\n","train_encodings = tokenizer(train_data['combined_text'].tolist(), truncation=True, padding=True, max_length=512)\n","val_encodings = tokenizer(val_data['combined_text'].tolist(), truncation=True, padding=True, max_length=512)\n","test_encodings = tokenizer(test_data['combined_text'].tolist(), truncation=True, padding=True, max_length=512)\n","\n","class SpoilerDataset(Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels is not None:\n","            item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings['input_ids'])\n","\n","# Create dataset objects\n","train_dataset = SpoilerDataset(train_encodings, train_data['label'].tolist())\n","val_dataset = SpoilerDataset(val_encodings, val_data['label'].tolist())\n","test_dataset = SpoilerDataset(test_encodings)\n","\n","# Load pre-trained BERT model for sequence classification\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n","\n","# Prepare the data loaders\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","# Set up the optimizer and learning rate scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_loader)\n","lr_scheduler = get_scheduler(\n","    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",")\n","\n","# Move the model to the GPU if available\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","\n","# Training loop\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_loader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","\n","# Validation loop\n","model.eval()\n","predictions = []\n","true_labels = []\n","for batch in val_loader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    logits = outputs.logits\n","    predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n","    true_labels.extend(batch['labels'].cpu().numpy())\n","\n","from sklearn.metrics import classification_report\n","print(classification_report(true_labels, predictions, target_names=label_encoder.classes_))\n","\n","# Make predictions on the test set\n","model.eval()\n","test_predictions = []\n","for batch in test_loader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    logits = outputs.logits\n","    test_predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n","\n","# Map numerical labels back to original tags\n","predicted_tags = label_encoder.inverse_transform(test_predictions)\n","\n","# Prepare predictions for submission\n","submission = pd.DataFrame({\n","    'id': range(len(test_data)),  # Use a range of integers as the IDs\n","    'spoilerType': predicted_tags\n","})\n","\n","# Save the predictions to a CSV file\n","submission.to_csv('/content/drive/MyDrive/task-1/data/prediction_task1.csv', index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312,"referenced_widgets":["233af25196414f58ae19927b3c42e607","7dc26461545849c487dab0eb4a211dba","166fd59fb4454e00b49a36a9ded68bb0","433c2a0aa56645eaba47210f27241bcf","2f0e6f8900514454a1dbbe5c7d714482","5680f8ee40a44c6aba7d6ec8631b64cc","1f7a7b09bcbc4db38d6f1f15fee43ab4","1509cac5352b484ea509d916330f1083","a5bb994181cb4eaeb89bc14f91f2446d","9e3dca0dbe194b0fb5fb31c94f190a5e","228a3802e4354cfd9137597e499a9b15"]},"id":"0YTmvfWlsqb7","executionInfo":{"status":"ok","timestamp":1719716610765,"user_tz":240,"elapsed":954870,"user":{"displayName":"MMA Construction","userId":"18335187591541982093"}},"outputId":"bb56be74-d479-4cd9-e161-3dfaf39a9be5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1200 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233af25196414f58ae19927b3c42e607"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","       multi       0.69      0.44      0.54        84\n","     passage       0.56      0.66      0.61       154\n","      phrase       0.58      0.59      0.58       162\n","\n","    accuracy                           0.58       400\n","   macro avg       0.61      0.56      0.58       400\n","weighted avg       0.59      0.58      0.58       400\n","\n"]}]}]}